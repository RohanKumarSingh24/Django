{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file with open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('rohan.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning — Text Processing\n",
      "\n",
      "Javaid Nabi\n",
      "\n",
      "Javaid Nabi\n",
      "\n",
      "Follow\n",
      "\n",
      "Sep 13, 2018 · 10 min read\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Processing is one of the most common task in many ML applications. Below are some examples of such applications.\n",
      "\n",
      "• Language Translation: Translation of a sentence from one language to another.\n",
      "\n",
      "• Sentiment Analysis: To determine, from a text corpus, whether the  sentiment towards any topic or product etc. is positive, negative, or neutral.\n",
      "\n",
      "• Spam Filtering:  Detect unsolicited and unwanted email/messages.\n",
      "\n",
      "Image for post\n",
      "\n",
      "Courtesy (sigmoidal)\n",
      "\n",
      "These applications deal with huge amount of text to perform classification or translation and involves a lot of work on the back end. Transforming text into something an algorithm can digest is a complicated process. In this article, we will discuss the steps involved in text processing.\n",
      "\n",
      "Step 1 : Data Preprocessing\n",
      "\n",
      "Tokenization — convert sentences to words\n",
      "\n",
      "Removing unnecessary punctuation, tags\n",
      "\n",
      "Removing stop words — frequent words such as ”the”, ”is”, etc. that do not have specific semantic\n",
      "\n",
      "Stemming — words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix.\n",
      "\n",
      "Lemmatization — Another approach to remove inflection by determining the part of speech and utilizing detailed database of the language.\n",
      "\n",
      "The stemmed form of studies is: studi\n",
      "\n",
      "The stemmed form of studying is: study\n",
      "\n",
      "The lemmatized form of studies is: study\n",
      "\n",
      "The lemmatized form of studying is: study\n",
      "\n",
      "Thus stemming & lemmatization help reduce words like ‘studies’, ‘studying’ to a common base form or root word ‘study’. For detailed discussion on Stemming & Lemmatization refer here . Note that not all the steps are mandatory and is based on the application use case. For Spam Filtering we may follow all the above steps but may not for language translation problem.\n",
      "\n",
      "We can use python to do many text preprocessing operations.\n",
      "\n",
      "NLTK — The Natural Language ToolKit is one of the best-known and most-used NLP libraries, useful for all sorts of tasks from t tokenization, stemming, tagging, parsing, and beyond\n",
      "\n",
      "BeautifulSoup — Library for extracting data from HTML and XML documents\n",
      "\n",
      "#using NLTK library, we can do lot of text preprocesing\n",
      "\n",
      "import nltk\n",
      "\n",
      "from nltk.tokenize import word_tokenize\n",
      "\n",
      "#function to split text into word\n",
      "\n",
      "tokens = word_tokenize(\"The quick brown fox jumps over the lazy dog\")\n",
      "\n",
      "nltk.download('stopwords')\n",
      "\n",
      "print(tokens)\n",
      "\n",
      "OUT: [‘The’, ‘quick’, ‘brown’, ‘fox’, ‘jumps’, ‘over’, ‘the’, ‘lazy’, ‘dog’]\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "stop_words = set(stopwords.words(‘english’))\n",
      "\n",
      "tokens = [w for w in tokens if not w in stop_words]\n",
      "\n",
      "print(tokens)\n",
      "\n",
      "OUT: [‘The’, ‘quick’, ‘brown’, ‘fox’, ‘jumps’, ‘lazy’, ‘dog’]\n",
      "\n",
      "#NLTK provides several stemmer interfaces like Porter stemmer, #Lancaster Stemmer, Snowball Stemmer\n",
      "\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "\n",
      "porter = PorterStemmer()\n",
      "\n",
      "stems = []\n",
      "\n",
      "for t in tokens:    \n",
      "\n",
      "    stems.append(porter.stem(t))\n",
      "\n",
      "print(stems)\n",
      "\n",
      "OUT: [‘the’, ‘quick’, ‘brown’, ‘fox’, ‘jump’, ‘lazi’, ‘dog’]\n",
      "\n",
      "Step 2: Feature Extraction\n",
      "\n",
      "In text processing, words of the text represent discrete, categorical features. How do we encode such data in a way which is ready to be used by the algorithms? The mapping from textual data to real valued vectors is called feature extraction. One of the simplest techniques to numerically represent text is Bag of Words.\n",
      "\n",
      "Bag of Words (BOW): We make the list of unique words in the text corpus called vocabulary. Then we can represent each sentence or document as a vector with each word represented as 1 for present and 0 for absent from the vocabulary. Another representation can be count the number of times each word appears in a document. The most popular approach is using the Term Frequency-Inverse Document Frequency (TF-IDF) technique.\n",
      "\n",
      "Term Frequency (TF) = (Number of times term t appears in a document)/(Number of terms in the document)\n",
      "\n",
      "Inverse Document Frequency (IDF) = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in. The IDF of a rare word is high, whereas the IDF of a frequent word is likely to be low. Thus having the effect of highlighting words that are distinct.\n",
      "\n",
      "We calculate TF-IDF value of a term as = TF * IDF\n",
      "\n",
      "Let us take an example to calculate TF-IDF of a term in a document.\n",
      "\n",
      "Image for post\n",
      "\n",
      "Example text corpus\n",
      "\n",
      "TF('beautiful',Document1) = 2/10, IDF('beautiful')=log(2/2) = 0\n",
      "\n",
      "TF(‘day’,Document1) = 5/10,  IDF(‘day’)=log(2/1) = 0.30\n",
      "\n",
      "\n",
      "\n",
      "TF-IDF(‘beautiful’, Document1) = (2/10)*0 = 0\n",
      "\n",
      "TF-IDF(‘day’, Document1) = (5/10)*0.30 = 0.15\n",
      "\n",
      "As, you can see for Document1 , TF-IDF method heavily penalizes the word ‘beautiful’ but assigns greater weight to ‘day’. This is due to IDF part, which gives more weightage to the words that are distinct. In other words, ‘day’ is an important word for Document1 from the context of the entire corpus. Python scikit-learn library provides efficient tools for text data mining and provides functions to calculate TF-IDF of text vocabulary given a text corpus.\n",
      "\n",
      "One of the major disadvantages of using BOW is that it discards word order thereby ignoring the context and in turn meaning of words in the document. For natural language processing (NLP) maintaining the context of the words is of utmost importance. To solve this problem we use another approach called Word Embedding.\n",
      "\n",
      "Word Embedding: It is a representation of text where words that have the same meaning have a similar representation. In other words it represents words in a coordinate system where related words, based on a\n"
     ]
    }
   ],
   "source": [
    "file=open('/mnt/c/users/rohan singh/Desktop/Ubantu/Python/rohan.txt','r')\n",
    "for t in file:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(file.read(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file with open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('rohan1.txt','w')\n",
    "f.write('Python is high level language\\n')\n",
    "f.write('Its support oops concept\\n')\n",
    "f.write('Python support multiple inheritance')\n",
    "f.close()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append with open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=open('rakesh.txt','a')\n",
    "a.write('dsnandaldlamd;lal;,d;,c;lv,;sc,v;,scv,;csv,ndflkflkdfkjadkjala;dk')\n",
    "a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using write along with with() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In python the with keyword is used when working with unmanaged resources (like file streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning — Text Processing\n",
      "Javaid Nabi\n",
      "Javaid Nabi\n",
      "Follow\n",
      "Sep 13, 2018 · 10 min read\n",
      "\n",
      "\n",
      "\n",
      "Text Processing is one of the most common task in many ML applications. Below are some examples of such applications.\n",
      "• Language Translation: Translation of a sentence from one language to another.\n",
      "• Sentiment Analysis: To determine, from a text corpus, whether the  sentiment towards any topic or product etc. is positive, negative, or neutral.\n",
      "• Spam Filtering:  Detect unsolicited and unwanted email/messages.\n",
      "Image for post\n",
      "Courtesy (sigmoidal)\n",
      "These applications deal with huge amount of text to perform classification or translation and involves a lot of work on the back end. Transforming text into something an algorithm can digest is a complicated process. In this article, we will discuss the steps involved in text processing.\n",
      "Step 1 : Data Preprocessing\n",
      "Tokenization — convert sentences to words\n",
      "Removing unnecessary punctuation, tags\n",
      "Removing stop words — frequent words such as ”the”, ”is”, etc. that do not have specific semantic\n",
      "Stemming — words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix.\n",
      "Lemmatization — Another approach to remove inflection by determining the part of speech and utilizing detailed database of the language.\n",
      "The stemmed form of studies is: studi\n",
      "The stemmed form of studying is: study\n",
      "The lemmatized form of studies is: study\n",
      "The lemmatized form of studying is: study\n",
      "Thus stemming & lemmatization help reduce words like ‘studies’, ‘studying’ to a common base form or root word ‘study’. For detailed discussion on Stemming & Lemmatization refer here . Note that not all the steps are mandatory and is based on the application use case. For Spam Filtering we may follow all the above steps but may not for language translation problem.\n",
      "We can use python to do many text preprocessing operations.\n",
      "NLTK — The Natural Language ToolKit is one of the best-known and most-used NLP libraries, useful for all sorts of tasks from t tokenization, stemming, tagging, parsing, and beyond\n",
      "BeautifulSoup — Library for extracting data from HTML and XML documents\n",
      "#using NLTK library, we can do lot of text preprocesing\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "#function to split text into word\n",
      "tokens = word_tokenize(\"The quick brown fox jumps over the lazy dog\")\n",
      "nltk.download('stopwords')\n",
      "print(tokens)\n",
      "OUT: [‘The’, ‘quick’, ‘brown’, ‘fox’, ‘jumps’, ‘over’, ‘the’, ‘lazy’, ‘dog’]\n",
      "from nltk.corpus import stopwords\n",
      "stop_words = set(stopwords.words(‘english’))\n",
      "tokens = [w for w in tokens if not w in stop_words]\n",
      "print(tokens)\n",
      "OUT: [‘The’, ‘quick’, ‘brown’, ‘fox’, ‘jumps’, ‘lazy’, ‘dog’]\n",
      "#NLTK provides several stemmer interfaces like Porter stemmer, #Lancaster Stemmer, Snowball Stemmer\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "porter = PorterStemmer()\n",
      "stems = []\n",
      "for t in tokens:    \n",
      "    stems.append(porter.stem(t))\n",
      "print(stems)\n",
      "OUT: [‘the’, ‘quick’, ‘brown’, ‘fox’, ‘jump’, ‘lazi’, ‘dog’]\n",
      "Step 2: Feature Extraction\n",
      "In text processing, words of the text represent discrete, categorical features. How do we encode such data in a way which is ready to be used by the algorithms? The mapping from textual data to real valued vectors is called feature extraction. One of the simplest techniques to numerically represent text is Bag of Words.\n",
      "Bag of Words (BOW): We make the list of unique words in the text corpus called vocabulary. Then we can represent each sentence or document as a vector with each word represented as 1 for present and 0 for absent from the vocabulary. Another representation can be count the number of times each word appears in a document. The most popular approach is using the Term Frequency-Inverse Document Frequency (TF-IDF) technique.\n",
      "Term Frequency (TF) = (Number of times term t appears in a document)/(Number of terms in the document)\n",
      "Inverse Document Frequency (IDF) = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in. The IDF of a rare word is high, whereas the IDF of a frequent word is likely to be low. Thus having the effect of highlighting words that are distinct.\n",
      "We calculate TF-IDF value of a term as = TF * IDF\n",
      "Let us take an example to calculate TF-IDF of a term in a document.\n",
      "Image for post\n",
      "Example text corpus\n",
      "TF('beautiful',Document1) = 2/10, IDF('beautiful')=log(2/2) = 0\n",
      "TF(‘day’,Document1) = 5/10,  IDF(‘day’)=log(2/1) = 0.30\n",
      "\n",
      "TF-IDF(‘beautiful’, Document1) = (2/10)*0 = 0\n",
      "TF-IDF(‘day’, Document1) = (5/10)*0.30 = 0.15\n",
      "As, you can see for Document1 , TF-IDF method heavily penalizes the word ‘beautiful’ but assigns greater weight to ‘day’. This is due to IDF part, which gives more weightage to the words that are distinct. In other words, ‘day’ is an important word for Document1 from the context of the entire corpus. Python scikit-learn library provides efficient tools for text data mining and provides functions to calculate TF-IDF of text vocabulary given a text corpus.\n",
      "One of the major disadvantages of using BOW is that it discards word order thereby ignoring the context and in turn meaning of words in the document. For natural language processing (NLP) maintaining the context of the words is of utmost importance. To solve this problem we use another approach called Word Embedding.\n",
      "Word Embedding: It is a representation of text where words that have the same meaning have a similar representation. In other words it represents words in a coordinate system where related words, based on a\n"
     ]
    }
   ],
   "source": [
    "with open('rohan.txt','r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rakesh.txt','a') as f:\n",
    "    f.write('rohan kumar singh')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsnandaldlamd;lal;,d;,c;lv,;sc,v;,scv,;csv,ndflkflkdfkjadkjala;dkdsnandaldlamd;lal;,d;,c;lv,;sc,v;,scv,;csv,ndflkflkdfkjadkjala;dkdsnandaldlamd;lal;,d;,c;lv,;sc,v;,scv,;csv,ndflkflkdfkjadkjala;dkrohan kumar singhrohan kumar singhrohan kumar singh\n"
     ]
    }
   ],
   "source": [
    "with open('rakesh.txt','r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning — Text Processing\n",
      "\n",
      "Javaid Nabi\n",
      "\n",
      "Javaid Nabi\n",
      "\n",
      "Follow\n",
      "\n",
      "Sep 13, 2018 · 10 min read\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text Processing is one of the most common task in many ML applications. Below are some examples of such applications.\n",
      "\n",
      "• Language Translation: Translation of a sentence from one language to another.\n",
      "\n",
      "• Sentiment Analysis: To determine, from a text corpus, whether the  sentiment towards any topic or product etc. is positive, negative, or neutral.\n",
      "\n",
      "• Spam Filtering:  Detect unsolicited and unwanted email/messages.\n",
      "\n",
      "Image for post\n",
      "\n",
      "Courtesy (sigmoidal)\n",
      "\n",
      "These applications deal with huge amount of text to perform classification or translation and involves a lot of work on the back end. Transforming text into something an algorithm can digest is a complicated process. In this article, we will discuss the steps involved in text processing.\n",
      "\n",
      "Step 1 : Data Preprocessing\n",
      "\n",
      "Tokenization — convert sentences to words\n",
      "\n",
      "Removing unnecessary punctuation, tags\n",
      "\n",
      "Removing stop words — frequent words such as ”the”, ”is”, etc. that do not have specific semantic\n",
      "\n",
      "Stemming — words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix.\n",
      "\n",
      "Lemmatization — Another approach to remove inflection by determining the part of speech and utilizing detailed database of the language.\n",
      "\n",
      "The stemmed form of studies is: studi\n",
      "\n",
      "The stemmed form of studying is: study\n",
      "\n",
      "The lemmatized form of studies is: study\n",
      "\n",
      "The lemmatized form of studying is: study\n",
      "\n",
      "Thus stemming & lemmatization help reduce words like ‘studies’, ‘studying’ to a common base form or root word ‘study’. For detailed discussion on Stemming & Lemmatization refer here . Note that not all the steps are mandatory and is based on the application use case. For Spam Filtering we may follow all the above steps but may not for language translation problem.\n",
      "\n",
      "We can use python to do many text preprocessing operations.\n",
      "\n",
      "NLTK — The Natural Language ToolKit is one of the best-known and most-used NLP libraries, useful for all sorts of tasks from t tokenization, stemming, tagging, parsing, and beyond\n",
      "\n",
      "BeautifulSoup — Library for extracting data from HTML and XML documents\n",
      "\n",
      "#using NLTK library, we can do lot of text preprocesing\n",
      "\n",
      "import nltk\n",
      "\n",
      "from nltk.tokenize import word_tokenize\n",
      "\n",
      "#function to split text into word\n",
      "\n",
      "tokens = word_tokenize(\"The quick brown fox jumps over the lazy dog\")\n",
      "\n",
      "nltk.download('stopwords')\n",
      "\n",
      "print(tokens)\n",
      "\n",
      "OUT: [‘The’, ‘quick’, ‘brown’, ‘fox’, ‘jumps’, ‘over’, ‘the’, ‘lazy’, ‘dog’]\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "stop_words = set(stopwords.words(‘english’))\n",
      "\n",
      "tokens = [w for w in tokens if not w in stop_words]\n",
      "\n",
      "print(tokens)\n",
      "\n",
      "OUT: [‘The’, ‘quick’, ‘brown’, ‘fox’, ‘jumps’, ‘lazy’, ‘dog’]\n",
      "\n",
      "#NLTK provides several stemmer interfaces like Porter stemmer, #Lancaster Stemmer, Snowball Stemmer\n",
      "\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "\n",
      "porter = PorterStemmer()\n",
      "\n",
      "stems = []\n",
      "\n",
      "for t in tokens:    \n",
      "\n",
      "    stems.append(porter.stem(t))\n",
      "\n",
      "print(stems)\n",
      "\n",
      "OUT: [‘the’, ‘quick’, ‘brown’, ‘fox’, ‘jump’, ‘lazi’, ‘dog’]\n",
      "\n",
      "Step 2: Feature Extraction\n",
      "\n",
      "In text processing, words of the text represent discrete, categorical features. How do we encode such data in a way which is ready to be used by the algorithms? The mapping from textual data to real valued vectors is called feature extraction. One of the simplest techniques to numerically represent text is Bag of Words.\n",
      "\n",
      "Bag of Words (BOW): We make the list of unique words in the text corpus called vocabulary. Then we can represent each sentence or document as a vector with each word represented as 1 for present and 0 for absent from the vocabulary. Another representation can be count the number of times each word appears in a document. The most popular approach is using the Term Frequency-Inverse Document Frequency (TF-IDF) technique.\n",
      "\n",
      "Term Frequency (TF) = (Number of times term t appears in a document)/(Number of terms in the document)\n",
      "\n",
      "Inverse Document Frequency (IDF) = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in. The IDF of a rare word is high, whereas the IDF of a frequent word is likely to be low. Thus having the effect of highlighting words that are distinct.\n",
      "\n",
      "We calculate TF-IDF value of a term as = TF * IDF\n",
      "\n",
      "Let us take an example to calculate TF-IDF of a term in a document.\n",
      "\n",
      "Image for post\n",
      "\n",
      "Example text corpus\n",
      "\n",
      "TF('beautiful',Document1) = 2/10, IDF('beautiful')=log(2/2) = 0\n",
      "\n",
      "TF(‘day’,Document1) = 5/10,  IDF(‘day’)=log(2/1) = 0.30\n",
      "\n",
      "\n",
      "\n",
      "TF-IDF(‘beautiful’, Document1) = (2/10)*0 = 0\n",
      "\n",
      "TF-IDF(‘day’, Document1) = (5/10)*0.30 = 0.15\n",
      "\n",
      "As, you can see for Document1 , TF-IDF method heavily penalizes the word ‘beautiful’ but assigns greater weight to ‘day’. This is due to IDF part, which gives more weightage to the words that are distinct. In other words, ‘day’ is an important word for Document1 from the context of the entire corpus. Python scikit-learn library provides efficient tools for text data mining and provides functions to calculate TF-IDF of text vocabulary given a text corpus.\n",
      "\n",
      "One of the major disadvantages of using BOW is that it discards word order thereby ignoring the context and in turn meaning of words in the document. For natural language processing (NLP) maintaining the context of the words is of utmost importance. To solve this problem we use another approach called Word Embedding.\n",
      "\n",
      "Word Embedding: It is a representation of text where words that have the same meaning have a similar representation. In other words it represents words in a coordinate system where related words, based on a\n"
     ]
    }
   ],
   "source": [
    "with open('rohan.txt','r') as f:\n",
    "    data=f.readlines()\n",
    "    #print(data)\n",
    "    for i in data:\n",
    "        spdata=i.split()\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to delete data from file in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: Rohan KumarRoll No: 240489Branch: CSEName: Rohan Kumar\n",
      "Roll No: 240489\n",
      "Branch: CSE\n",
      "Name: Rohan Kumar\n",
      "Roll No: 240489\n",
      "Branch: CSE\n",
      "Name: Rohan Kumar\n",
      "Roll No: 240489\n",
      "Branch: CSE\n",
      "Name: Rohan Kumar\n",
      "Roll No: 240489\n",
      "Branch: CSE\n",
      "Name: Rohan Kumar\n",
      "Roll No: 240489\n",
      "Branch: CSE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('rakesh1.txt','r+') as f:\n",
    "    f.write('Name: Rohan Kumar\\n')\n",
    "    f.write('Roll No: 240489\\n')\n",
    "    f.write('Branch: CSE\\n')\n",
    "    \n",
    "    print(f.read())\n",
    "    f.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.exists('rakesh.txt'):\n",
    "    os.remove('rakesh.txt')\n",
    "    print('File exist/Deleted')\n",
    "else:\n",
    "    print('File does not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: When the entire data has to be deleted but not the file it is in !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'/mnt/c/users/rohan singh/Desktop/Ubantu/Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diractories=os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "class_file.py\n",
      "Flask\n",
      "nba.csv\n",
      "Pthon-Set.ipynb\n",
      "Python-Array.ipynb\n",
      "Python-Control_Flow.ipynb\n",
      "Python-Dictionary-Problems.ipynb\n",
      "Python-Dictionary.ipynb\n",
      "Python-File-Handling.ipynb\n",
      "Python-Function.ipynb\n",
      "Python-list.ipynb\n",
      "Python-Numpy.ipynb\n",
      "Python-Object-Oriented-Concept.ipynb\n",
      "Python-Operators.ipynb\n",
      "Python-Pandas.ipynb\n",
      "Python-Tuple.ipynb\n",
      "PythonBasic-String.ipynb\n",
      "rakesh1.txt\n",
      "rohan.txt\n",
      "rohan1.txt\n",
      "test_file.py\n",
      "Untitled.ipynb\n",
      "Untitled1.ipynb\n",
      "__pycache__\n"
     ]
    }
   ],
   "source": [
    "for i in diractories:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class_file.py', 'nba.csv', 'Pthon-Set.ipynb', 'Python-Array.ipynb', 'Python-Control_Flow.ipynb', 'Python-Dictionary-Problems.ipynb', 'Python-Dictionary.ipynb', 'Python-File-Handling.ipynb', 'Python-Function.ipynb', 'Python-list.ipynb', 'Python-Numpy.ipynb', 'Python-Object-Oriented-Concept.ipynb', 'Python-Operators.ipynb', 'Python-Pandas.ipynb', 'Python-Tuple.ipynb', 'PythonBasic-String.ipynb', 'rakesh1.txt', 'rohan.txt', 'rohan1.txt', 'test_file.py', 'Untitled.ipynb', 'Untitled1.ipynb']\n",
      "['Pthon-Set-checkpoint.ipynb', 'Python-Array-checkpoint.ipynb', 'Python-Control_Flow-checkpoint.ipynb', 'Python-Dictionary-checkpoint.ipynb', 'Python-Dictionary-Problems-checkpoint.ipynb', 'Python-File-Handling-checkpoint.ipynb', 'Python-Function-checkpoint.ipynb', 'Python-list-checkpoint.ipynb', 'Python-Numpy-checkpoint.ipynb', 'Python-Object-Oriented-Concept-checkpoint.ipynb', 'Python-Operators-checkpoint.ipynb', 'Python-Pandas-checkpoint.ipynb', 'Python-Tuple-checkpoint.ipynb', 'PythonBasic-String-checkpoint.ipynb', 'Untitled-checkpoint.ipynb', 'Untitled1-checkpoint.ipynb']\n",
      "['Flask.ipynb']\n",
      "['Flask-checkpoint.ipynb']\n",
      "['class_file.cpython-38.pyc', 'test_file.cpython-38.pyc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/mnt/c/users/rohan singh/Desktop/Ubantu/Python/rakesh1.txt',\n",
       " '/mnt/c/users/rohan singh/Desktop/Ubantu/Python/rohan.txt',\n",
       " '/mnt/c/users/rohan singh/Desktop/Ubantu/Python/rohan1.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, fnmatch\n",
    "def find(pattern, path):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        print(files)\n",
    "        for name in files:\n",
    "            if fnmatch.fnmatch(name, pattern):\n",
    "                result.append(os.path.join(root, name))\n",
    "    return result\n",
    "find('*.txt',r'/mnt/c/users/rohan singh/Desktop/Ubantu/Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rakesh1.txt', 'rohan.txt', 'rohan1.txt']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "glob.glob('*.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rakesh1.txt', 'rohan.txt', 'rohan1.txt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=r'/mnt/c/users/rohan singh/Desktop/Ubantu/Python'\n",
    "text_files = [f for f in os.listdir(path) if f.endswith('.txt')]\n",
    "text_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rakesh1.txt\n",
      "rohan.txt\n",
      "rohan1.txt\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "os.chdir(r'/mnt/c/users/rohan singh/Desktop/Ubantu/Python')\n",
    "for file in glob.glob(\"*.txt\"):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/users/rohan singh/Desktop/Ubantu/Python/rakesh1.txt\n",
      "/mnt/c/users/rohan singh/Desktop/Ubantu/Python/rohan.txt\n",
      "/mnt/c/users/rohan singh/Desktop/Ubantu/Python/rohan1.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path=r'/mnt/c/users/rohan singh/Desktop/Ubantu/Python'\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        print(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/users/rohan singh/Desktop/Ubantu/Python/rakesh1.txt\n",
      "/mnt/c/users/rohan singh/Desktop/Ubantu/Python/rohan.txt\n",
      "/mnt/c/users/rohan singh/Desktop/Ubantu/Python/rohan1.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        print(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "h\n",
      "n\n",
      "o\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path1='/mnt/c/users/rohan singh/Desktop/Ubantu/r'\n",
    "for file in os.listdir(path1):\n",
    "    print(file)\n",
    "    if file.endswith('.txt'):\n",
    "        print(os.path1.join(path1,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/c/users/rohan singh/Desktop/Ubantu/r']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "f=glob.glob(path1)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/users/rohan singh/Desktop/Ubantu/r/a\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/mnt/c/users/rohan singh/Desktop/Ubantu/r/a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5245f95136cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/mnt/c/users/rohan singh/Desktop/Ubantu/r/a'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "for infile in glob.glob(os.path.join(path1, '*')):\n",
    "    print(infile)\n",
    "    f=open(infile,'r').read()\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'h', 'n', 'o', 'r']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "dire=os.listdir(path1)\n",
    "dire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "h\n",
      "n\n",
      "o\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "#os.scandir(path1)\n",
    "with os.scandir(path1) as f:\n",
    "    for i in f:\n",
    "        print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'in_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-07f05bd62fc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m data_paths = [os.path.join(path1, f) \n\u001b[0;32m----> 2\u001b[0;31m     for path1, dirs, files in os.walk(in_dir) for f in files]\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'in_dir' is not defined"
     ]
    }
   ],
   "source": [
    "data_paths = [os.path.join(path1, f) \n",
    "    for path1, dirs, files in os.walk(in_dir) for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/users/rohan singh/Desktop/Ubantu/Python'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-549271837d3e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-549271837d3e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for root in os.walk(path1)\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for root in os.walk(path1) \n",
    "    for dir in root \n",
    "        for files in dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/mnt/c/users/rohan singh/Desktop/Ubantu/r/a', [], ['rakesh.txt'])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ec2307a73b28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#             print(dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not tuple"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "for infile in glob.glob(os.path.join(path1, '*')):\n",
    "    #print(infile)\n",
    "    for root in os.walk(infile):\n",
    "        print(root)\n",
    "#         for dir in root:\n",
    "#             print(dir)\n",
    "            \n",
    "    f=open(root,'r').read()\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
